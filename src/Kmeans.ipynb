{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import plotly.express as px\n",
    "\n",
    "# Pipeline imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer, StandardScaler, MinMaxScaler, PCA\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import count, when, isnan, col, log1p, log, create_map, lit\n",
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName(\"BD\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical(data):\n",
    "    \"\"\"\n",
    "    Returns a list of column names that have numerical data types (int or double).\n",
    "    \"\"\"\n",
    "    return [t[0] for t in data.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "\n",
    "\n",
    "def plot_hist(data, col):\n",
    "    fig = px.histogram(data.toPandas(), x=col)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def clean(data):\n",
    "    \"\"\"\n",
    "    Cleans the given data by filling missing values, dropping rows with missing values,\n",
    "    and mapping categorical variables to numerical values.\n",
    "    \"\"\"\n",
    "    data = data.fillna(0, subset=[\"tempo\"])\n",
    "    data = data.dropna()\n",
    "\n",
    "    keys = {'A': 0, 'A#': 1, 'B': 2, 'C': 3, 'C#': 4, 'D': 5, 'D#': 6, 'E': 7, 'F': 8, 'F#': 9, 'G': 10, 'G#': 11}\n",
    "    key_mapping = create_map([lit(x) for x in chain(*keys.items())])\n",
    "    data = data.withColumn(\"key\", key_mapping[col(\"key\")])\n",
    "\n",
    "    modes = { 'Minor': 0, 'Major': 1 }\n",
    "    mode_mapping = create_map([lit(x) for x in chain(*modes.items())])\n",
    "    data = data.withColumn(\"mode\", mode_mapping[col(\"mode\")])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def norm_data(data, col, mean=None, std=None, maxv=None, minv=None):\n",
    "    \"\"\"\n",
    "    Normalize the data in the specified column within the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): The input DataFrame.\n",
    "        col (str): The name of the column to normalize.\n",
    "        mean (float, optional): The mean value to use for normalization. If not provided, it will be calculated from the data.\n",
    "        std (float, optional): The standard deviation value to use for normalization. If not provided, it will be calculated from the data.\n",
    "        maxv (float, optional): The maximum value to use for normalization. If not provided, it will be calculated from the data.\n",
    "        minv (float, optional): The minimum value to use for normalization. If not provided, it will be calculated from the data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataFrame, float, float, float, float]: A tuple containing the normalized DataFrame, mean, standard deviation, maximum value, and minimum value.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if mean is None: mean = data.agg({col: \"mean\"}).collect()[0][0]\n",
    "    if std is None: std = data.agg({col: \"stddev\"}).collect()[0][0]\n",
    "\n",
    "    # outlier\n",
    "    data = data.withColumn(f\"{col}_outlier\", when((data[col] < mean - 3 * std) | (data[col] > mean + 3 * std), 1).otherwise(0))\n",
    "    data = data.withColumn(col, when(data[col] < mean - 3 * std, mean - 3 * std).otherwise(data[col]))\n",
    "    data = data.withColumn(col, when(data[col] > mean + 3 * std, mean + 3 * std).otherwise(data[col]))\n",
    "\n",
    "    # normalize [0,1] range\n",
    "    if maxv is None: maxv = data.agg({col: \"max\"}).collect()[0][0]\n",
    "    if minv is None: minv = data.agg({col: \"min\"}).collect()[0][0]\n",
    "    data = data.withColumn(f\"{col}\", (data[col] - minv) / (maxv - minv))\n",
    "\n",
    "    # delete outlier column\n",
    "    data = data.drop(f\"{col}_outlier\")\n",
    "\n",
    "    return data, mean, std, maxv, minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [\"music_genre\"]\n",
    "seed = 1235\n",
    "data = spark.read.csv(\"../data/training.csv\", header=True, inferSchema=True, nullValue=\"\", sep=\";\")\n",
    "data = clean(data)\n",
    "dev, _ = data.randomSplit([0.9, 0.1], seed=seed)\n",
    "\n",
    "\n",
    "# dev processing\n",
    "col_params = {}\n",
    "for col_name in get_numerical(dev):\n",
    "    dev, mean, std, maxv, minv = norm_data(dev, col_name)\n",
    "    col_params[col_name] = (mean, std, maxv, minv)\n",
    "    # plot_hist(dev, col_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"music_genre_index\"\n",
    "indexer = StringIndexer(inputCols=label, outputCols=[label_col])\n",
    "assembler = VectorAssembler(inputCols=get_numerical(dev), outputCol=\"features\")\n",
    "estimator = KMeans(k=10, featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[indexer, assembler, estimator])\n",
    "\n",
    "# params = (ParamGridBuilder()\n",
    "#     .build())\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "# cross = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# build and train the pipeline\n",
    "model = pipeline.fit(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.40411414, 0.82223417, 0.39526801, 0.37891885, 0.22846783,\n",
      "       0.04075774, 0.47864611, 0.25800681, 0.40910916, 1.        ,\n",
      "       0.06839788, 0.4405602 , 0.2517767 ]), array([0.33816082, 0.85739039, 0.33081663, 0.42228986, 0.18024557,\n",
      "       0.84781475, 0.51000855, 0.19815319, 0.25083758, 1.        ,\n",
      "       0.0663467 , 0.4339261 , 0.23744101]), array([0.49773122, 0.58010529, 0.60715336, 0.31379181, 0.49343069,\n",
      "       0.04521976, 0.50977178, 0.24324815, 0.57759172, 1.        ,\n",
      "       0.14571375, 0.50910636, 0.59285126]), array([0.52181313, 0.16663399, 0.57455952, 0.36168117, 0.70855574,\n",
      "       0.02495552, 0.47865124, 0.27199471, 0.67283745, 0.        ,\n",
      "       0.1191819 , 0.49841731, 0.49853797]), array([0.59470727, 0.20046555, 0.69390008, 0.32995955, 0.66623092,\n",
      "       0.01389964, 0.4302926 , 0.30764568, 0.66348298, 0.        ,\n",
      "       0.69495024, 0.49052978, 0.51418818]), array([0.39819476, 0.18378586, 0.58037063, 0.40598522, 0.66176535,\n",
      "       0.76204844, 0.44587368, 0.26287973, 0.60085116, 0.        ,\n",
      "       0.109363  , 0.49963287, 0.45969681]), array([0.35786677, 0.89659775, 0.34355433, 0.38301512, 0.17937669,\n",
      "       0.57048331, 0.46534124, 0.20989822, 0.28939081, 0.        ,\n",
      "       0.07015883, 0.43254076, 0.23610877]), array([0.51249201, 0.13202403, 0.5686541 , 0.35813599, 0.71385877,\n",
      "       0.10297249, 0.51307692, 0.29860165, 0.67243739, 1.        ,\n",
      "       0.18367416, 0.00498112, 0.49949075]), array([0.52184471, 0.09179798, 0.58087398, 0.35331796, 0.74148572,\n",
      "       0.0940101 , 0.27135186, 0.2952151 , 0.68281008, 1.        ,\n",
      "       0.22200011, 0.57205973, 0.50296274]), array([0.5148532 , 0.10210943, 0.5622897 , 0.35508647, 0.73682444,\n",
      "       0.08323939, 0.85745599, 0.28550739, 0.68046627, 1.        ,\n",
      "       0.19115673, 0.566836  , 0.52060649])]\n"
     ]
    }
   ],
   "source": [
    "_, test = data.randomSplit([0.9, 0.1], seed=seed)\n",
    "\n",
    "# test processing\n",
    "for col_name in get_numerical(test):\n",
    "    test, _, _, _, _ = norm_data(test, col_name, *col_params[col_name])\n",
    "    # plot_hist(test, col_name)\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "centroids = model.stages[-1].clusterCenters()\n",
    "print(centroids)\n",
    "# use the centroids to predict the cluster for each data point\n",
    "predictions = model.transform(test)\n",
    "predictions = predictions.withColumn(\"prediction\", predictions[\"prediction\"].cast(\"double\"))\n",
    "predictions = predictions.withColumn(label_col, predictions[label_col].cast(\"double\"))\n",
    "# convert features to a vector\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# Define a UDF to convert Vector to Array\n",
    "vector_to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "predictions = predictions.withColumn(\"features_arr\", vector_to_array(predictions[\"features\"]))\n",
    "\n",
    "# display(predictions.limit(10).toPandas())\n",
    "# calculate the distance between each data point and the corresponding cluster centroid\n",
    "for i in range(len(centroids)):\n",
    "    predictions = predictions.withColumn(f\"distance_{i}\", sum((predictions[\"features_arr\"][j] - centroids[i][j]) ** 2 for j in range(len(centroids[i]))))\n",
    "\n",
    "display(predictions.limit(10).toPandas())\n",
    "# # calculate whether the prediction is correct\n",
    "# predictions = predictions.withColumn(\"is_correct\", when(test(\"prediction\") == test(label_col), 1).otherwise(0))\n",
    "\n",
    "# # calculate the accuracy\n",
    "# accuracy = predictions.agg({\"is_correct\": \"mean\"}).collect()[0][0]\n",
    "# print(f\"accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# Define a UDF to convert Vector to Array\n",
    "vector_to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "predictions = predictions.withColumn(\"features_arr\", vector_to_array(predictions[\"features\"]))\n",
    "\n",
    "# Now you can access the elements of features\n",
    "for i in range(len(centroids)):\n",
    "    predictions = predictions.withColumn(f\"distance_{i}\", sum((predictions[\"features_arr\"][j] - centroids[i][j]) ** 2 for j in range(len(centroids[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test count:  3029\n",
      "test count:  3029\n",
      "f1: 0.5869630239773582\n"
     ]
    }
   ],
   "source": [
    "_, test = data.randomSplit([0.9, 0.1], seed=seed)\n",
    "print(\"test count: \", test.count())\n",
    "\n",
    "# test processing\n",
    "for col_name in get_numerical(test):\n",
    "    test, _, _, _, _ = norm_data(test, col_name, *col_params[col_name])\n",
    "    # plot_hist(test, col_name)\n",
    "\n",
    "print(\"test count: \", test.count())\n",
    "\n",
    "# evaluate the model\n",
    "predictions = model.transform(test)\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(f\"f1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity: 8\n",
      "acousticness: 0\n",
      "danceability: 0\n",
      "duration_ms: 284\n",
      "energy: 0\n",
      "instrumentalness: 0\n",
      "liveness: 834\n",
      "loudness: 679\n",
      "speechiness: 631\n",
      "tempo: 0\n",
      "valence: 0\n",
      "key_num: 0\n",
      "mode_num: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in numerical_features:\n",
    "    mean = data.agg({col: \"mean\"}).collect()[0][0]\n",
    "    std = data.agg({col: \"stddev\"}).collect()[0][0]\n",
    "    # print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n",
    "    # use mean a std to calculate outliers\n",
    "    data = data.withColumn(f\"{col}_outlier\", when((data[col] < mean - 3 * std) | (data[col] > mean + 3 * std), 1).otherwise(0))\n",
    "    outliers = data.filter(f\"{col}_outlier = 1\")\n",
    "    print(f\"{col}: {outliers.count()}\")\n",
    "\n",
    "    # correct outliers\n",
    "    # data = data.withColumn(col, when(data[col] < mean - 3 * std, mean - 3 * std).otherwise(data[col]))\n",
    "    # data = data.withColumn(col, when(data[col] > mean + 3 * std, mean + 3 * std).otherwise(data[col]))\n",
    "\n",
    "    # remove outliers\n",
    "    data = data.filter(f\"{col}_outlier = 0\")\n",
    "    \n",
    "    # set outliers to mean\n",
    "    # data = data.withColumn(col, when(data[f\"{col}_outlier\"] == 1, mean).otherwise(data[col]))\n",
    "\n",
    "    # center in 0\n",
    "    # data = data.withColumn(f\"{col}_norm\", (data[col] - mean) / std)\n",
    "\n",
    "    # between 1 and 0\n",
    "    max = data.agg({col: \"max\"}).collect()[0][0]\n",
    "    min = data.agg({col: \"min\"}).collect()[0][0]\n",
    "    data = data.withColumn(f\"{col}_norm\", (data[col] - min) / (max - min))\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Program Files\\Blender Foundation\\Blender 4.0\\4.0\\python\\lib\\multiprocessing\\pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m cross \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mpipeline, estimatorParamMaps\u001b[38;5;241m=\u001b[39mparams, evaluator\u001b[38;5;241m=\u001b[39mevaluator, numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# build and train the pipeline\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcross\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test)\n",
      "File \u001b[1;32mc:\\Users\\edoelas\\git\\MUIARFID\\BD\\.venv\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\edoelas\\git\\MUIARFID\\BD\\.venv\\lib\\site-packages\\pyspark\\ml\\tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[0;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[1;32mC:\\Program Files\\Blender Foundation\\Blender 4.0\\4.0\\python\\lib\\multiprocessing\\pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[1;32mC:\\Program Files\\Blender Foundation\\Blender 4.0\\4.0\\python\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4554741183921099"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.agg({\"valence\": \"mean\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
