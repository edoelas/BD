{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.sql.functions import when, col, create_map, lit\n",
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName(\"BD\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical(data):\n",
    "    \"\"\"\n",
    "    Returns a list of column names that have numerical data types (int or double).\n",
    "    \"\"\"\n",
    "    return [t[0] for t in data.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "\n",
    "\n",
    "def clean(data):\n",
    "    \"\"\"\n",
    "    Cleans the given data by filling missing values, dropping rows with missing values,\n",
    "    and mapping categorical variables to numerical values.\n",
    "    \"\"\"\n",
    "    data = data.fillna(0, subset=[\"tempo\"])\n",
    "    data = data.dropna()\n",
    "\n",
    "    keys = {'A': 0, 'A#': 1, 'B': 2, 'C': 3, 'C#': 4, 'D': 5, 'D#': 6, 'E': 7, 'F': 8, 'F#': 9, 'G': 10, 'G#': 11}\n",
    "    key_mapping = create_map([lit(x) for x in chain(*keys.items())])\n",
    "    data = data.withColumn(\"key\", key_mapping[col(\"key\")])\n",
    "\n",
    "    modes = { 'Minor': 0, 'Major': 1 }\n",
    "    mode_mapping = create_map([lit(x) for x in chain(*modes.items())])\n",
    "    data = data.withColumn(\"mode\", mode_mapping[col(\"mode\")])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def norm_data(data, col, mean=None, std=None, maxv=None, minv=None):\n",
    "    \"\"\"\n",
    "    Normalize the data in the specified column within the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): The input DataFrame.\n",
    "        col (str): The name of the column to normalize.\n",
    "        mean (float, optional): The mean value to use for normalization. If not provided, it will be calculated from the data.\n",
    "        std (float, optional): The standard deviation value to use for normalization. If not provided, it will be calculated from the data.\n",
    "        maxv (float, optional): The maximum value to use for normalization. If not provided, it will be calculated from the data.\n",
    "        minv (float, optional): The minimum value to use for normalization. If not provided, it will be calculated from the data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataFrame, float, float, float, float]: A tuple containing the normalized DataFrame, mean, standard deviation, maximum value, and minimum value.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if mean is None: mean = data.agg({col: \"mean\"}).collect()[0][0]\n",
    "    if std is None: std = data.agg({col: \"stddev\"}).collect()[0][0]\n",
    "\n",
    "    # outlier\n",
    "    data = data.withColumn(f\"{col}_outlier\", when((data[col] < mean - 3 * std) | (data[col] > mean + 3 * std), 1).otherwise(0))\n",
    "    data = data.withColumn(col, when(data[col] < mean - 3 * std, mean - 3 * std).otherwise(data[col]))\n",
    "    data = data.withColumn(col, when(data[col] > mean + 3 * std, mean + 3 * std).otherwise(data[col]))\n",
    "\n",
    "    # normalize [0,1] range\n",
    "    if maxv is None: maxv = data.agg({col: \"max\"}).collect()[0][0]\n",
    "    if minv is None: minv = data.agg({col: \"min\"}).collect()[0][0]\n",
    "    data = data.withColumn(f\"{col}\", (data[col] - minv) / (maxv - minv))\n",
    "\n",
    "    # delete outlier column\n",
    "    data = data.drop(f\"{col}_outlier\")\n",
    "\n",
    "    return data, mean, std, maxv, minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process data\n",
    "label = [\"music_genre\"]\n",
    "seed = 1235\n",
    "data = spark.read.csv(\"../data/training.csv\", header=True, inferSchema=True, nullValue=\"\", sep=\";\")\n",
    "data = clean(data)\n",
    "dev, test = data.randomSplit([0.9, 0.1], seed=seed)\n",
    "\n",
    "# dev processing\n",
    "col_params = {}\n",
    "for col_name in get_numerical(dev):\n",
    "    dev, mean, std, maxv, minv = norm_data(dev, col_name)\n",
    "    col_params[col_name] = (mean, std, maxv, minv)\n",
    "\n",
    "# test processing\n",
    "for col_name in get_numerical(test):\n",
    "    test, _, _, _, _ = norm_data(test, col_name, *col_params[col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and train the pipeline\n",
    "label_col = \"music_genre_index\"\n",
    "indexer = StringIndexer(inputCols=label, outputCols=[label_col])\n",
    "assembler = VectorAssembler(inputCols=get_numerical(dev), outputCol=\"features\")\n",
    "estimator = MultilayerPerceptronClassifier(featuresCol=\"features\", labelCol=label_col)\n",
    "pipeline = Pipeline(stages=[indexer, assembler, estimator])\n",
    "\n",
    "params = (ParamGridBuilder()\n",
    "    .addGrid(estimator.layers, [[13, 20, 20, 10]])\n",
    "    .addGrid(estimator.maxIter, [1000])\n",
    "    .addGrid(estimator.blockSize, [512])\n",
    "    .build())\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "cross = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "model = cross.fit(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test count:  3029\n",
      "test count:  3029\n",
      "f1: 0.5869630239773582\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "predictions = model.transform(test)\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(f\"f1: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
